\documentclass[10pt]{article}
\usepackage[margin=26mm]{geometry}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathbbol}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{url}
%\usepackage{floatflt}
%\usepackage{mathptmx}
\usepackage{epsfig}
\usepackage{wrapfig}
\usepackage{color}

\usepackage{subfig}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}

% \usepackage{ccaption}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\S}{\mathbb{S}}
%\newcommand{\Wlog}{W.l.o.g.\ }
%\newcommand{\wlog}{w.l.o.g.\ }
\newcommand{\eps}{\epsilon}
\newcommand{\orient}{O}
\newcommand{\cpro}{\Pi}
\newcommand{\fib}{\mathrm{Fib}}

\newcommand{\parent}{\mathrm{par}}
\newcommand{\point}{p}
\newcommand{\level}{\ell}
\newcommand{\diameter}{\mathrm{diam}}
\newcommand{\pointset}{P}
\newcommand{\metricspace}{\mathcal{M}}
\newcommand{\metric}{\delta}
\newcommand{\approxmetric}{\gamma}
\newcommand{\complexity}{C_{\metric}}
\newcommand{\doublingdimension}{\Delta}

\newcommand{\remark}[1]{\textbf{[#1]}}

%\newtheorem{theorem}{Theorem}
%\newdef{definition}{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\myparagraph}[1]{\textbf{#1.}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
%\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}
\newtheorem{problem}{Problem}

%\numberwithin{equation}{section}
%\numberwithin{figure}{section}

\def\marrow{\marginpar[\hfill$\longrightarrow$]{$\longleftarrow$}}
\def\michael#1{\textcolor{red}{\textsc{Michael says: }{\marrow\sf #1}}}

\title{Geometric Filters for Metric Spaces}
\author{Michael Kerber, Arnur Nigmetov}
%
\date{\today}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{Motivation.}
%
Given a set $\pointset:=\{p_1,\ldots,p_n\}$ of $n$ points
in a metric space $(\metricspace,\metric)$, 
consider the following standard operations:

\begin{description}
\item[Nearest Neighbor] Given a point $q\in\metricspace$,
find $p_i\in\pointset$ such that, for all $j=1,\ldots,n$,
\[d(q,p_i)\leq d(q,p_j)\]

\item[Metric approximation] Given $\eps>0$, compute a metric $\approxmetric$
such that for all $1\leq i,j\leq n$,
%
\[\metric(p_i,p_j)\leq\approxmetric(p_i,p_j)\leq (1+\eps)\metric(p_i,p_j).\]

By ``computing'' $\approxmetric$, we mean to compute a $n\times n$-matrix $E$ 
such that $E_{ij}=\approxmetric(p_i,p_j)$.
\end{description}

Assume that an algorithm is given to compute $\metric(\cdot,\cdot)$
for any two points in $\metricspace$ in worst-case complexity $\complexity$.
Then trivially, the above problems can be solved (exactly) in
$O(n\complexity)$ and $O(n^2\complexity)$ time, respectively.
For low-dimensional\footnote{we will make this notion precise
later in the text} metric spaces, we can improve these bounds:
Using cover trees~\cite{cover-trees}, we can preprocess the point set
in $O(\complexity n\log n)$ time to answer nearest neighbor queries
in $O(\complexity \log n)$ time. Combining cover trees with well-separated
pair decompositions (WSPD), we obtain an $\eps$-spanner for the graph
induced by $\metric$ in $O(\complexity n\log n)$ time and
of linear size (assuming, for simplicity, that $\eps$ is a constant).
Out of this spanner,
we can construct an approximate metric
with an all-pair shortest path computation,
resulting in a total running time of
\[O(\complexity n\log n+ n^2\log n)\]
using Johnson's algorithm~\cite{johnson}.

In the aforementioned techniques, $\complexity$ is usually considered
as a constant-time operation and hence ignored in the analysis.
There are good reasons for that: the most common case of a metric space
is $\metricspace=\R^d$ with $d$ some constant, in which case $\metric$ can be evaluated in $O(d)=O(1)$ time.
Even if $d$ is considered non-constant,
it can always be assumed that $d\leq n$, hence $\complexity$ is at most $O(n)$.
Another typical assumption is that all pairwise distances are part of the input
in which case $\complexity$ is $O(1)$.

However, we argue that in some situations, distance computations
in $\metricspace$ can be costly and $\complexity$ might be incomparable
with $n$. Our motivating examples come from topological summaries
such as persistence diagrams or Reeb graphs, which are of interest
in the field of topological data analysis. A persistence diagram
is a point set in $\R^2$, and the distance between two diagrams
is determined by a min-cost matching between the point sets.
If the diagrams have $N$ points, computing this matching requires
polynomial time in $N$, and $N$ might well be larger than $n$, the number
of diagrams considered. For the case of Reeb graphs, the situation is even
worse: while several metrics on Reeb graphs have been proposed,
not even an constant-factor approximation algorithm is known that runs
in polynomial time in the size of the graphs.

In the light of the above examples, we pose the question: 
how can we practically reduce the number of distance computations 
in algorithms for metric spaces?
Note that for low-dimensional spaces, the aforementioned techniques
already provide a partial solution: for approximating the metric, for instance,
we only compute $O(n\log n)$ distances instead of $O(n^2)$.


\paragraph{Contribution.}
%
We demonstrate that the number of distance computations can be further
reduced by a careful re-implementation of the underlying geometric algorithms.
Our improvement is based on the following simple observation: many geometric
constructions (such as cover tree and WSPD constructions) require distance
computations only in the form of the following predicate: 
``Is $\metric(p_i,p_j)\leq t$'', where $t$ is a real value.
We show that using the information of the already computed distances and
the triangle inequality, we can answer the above predicate in many cases
without computing the actual distance.

We describe our idea with an example. In Figure~\ref{fig:1st_example},
we see a weighted graph on $P$ whose edges are the distances that have
been computed so far; note that $\metric(p_1,p_2)$ is unknown. 
The shortest path from $p_1$ to $p_2$ has length $9$, which clearly
constitutes an upper bound on the distance by triangle inequality.
However, we can also infer that $\metric(p_1,p_2)\geq 3$:
otherwise, the path from $p_3$ to $p_4$ via $p_1$ and $p_2$
would be shorter than the edge $(p_3,p_4)$, again contradicting
triangle inequality.
Hence, the question whether $\metric(p_1,p_2)\leq t$ can be answered positively
for $t>=9$ and negatively for $t<3$. Only if $t\in[3,9)$, the actual
distance computation is necessary. This approach is reminiscent to the usage
of \emph{numerical filters} in geometric computing in the CGAL library,
where geometric predicates in exact number types are first evaluated using 
certified floating-point arithmetic, and exact computation is only
invoked if the numerical evaluation did not arrive at a decisive result
(e.g.~\cite{bbp-interval,kerber-phd}).

\begin{figure}[h]

\centering
\includegraphics[width=6cm]{intro_example.eps}
\caption{An example of a partially revealed metric.}
\label{fig:1st_example}
\end{figure}

In our implementation, we maintain the best lower and upper bounds of all
pairwise distances in an $n\times n$ matrix of intervals. We show how to
update this matrix in $O(n^2)$ time when a new edge is added.
%We point out once more that a quadratic update time only pays off
%if distance computations are expensive, say $\complexity=\Omega(n^2)$.

We perform an extensive experimental evaluation of our idea.
In the first part, we concentrate on point clouds in Euclidean space~--
of course, the approach does not yield any performance benefits in that
situation because distance computations are cheap; however, the Euclidean
case provides a simple setup for experiments.
We investigate how the dimensionality, the clustering density, 
the approximation quality, and noise 
in the point cloud affect the number of distance computations.
Our result show that $\ldots$

We also provide experimental results for the case of persistence diagrams
which motivated this line of research. We point out that the space of
persistence diagrams fails to be ``low-dimensional''. To be precise,
its doubling dimension is $\infty$, and a finite set of $n$ diagram
can have the worst-case doublng dimension $\log n$. This worst-case
behavior is also the expected one if the diagrams are chosen randomly
(in a sense made precise in Section~\ref{sec:metric_spaces_top_summaries}). 
However, we demonstrate that
for natural collections of persistence diagrams, a substantial fraction
of distance computations can be avoided. In detail, $\ldots$

We also have derived two minor technical results necessary for 
our implementation. First of all, since our underlying distance computation
algorithm for persistence diagrams only returns approximate answers,
we had to adapt all algorithms to cope with approximate distances.
Moreover, we show how a WSPDs can be constructed
out of cover trees (we only found 
the theoretically more efficient version of
net trees used in the literature, but net trees are more difficult to implement
efficiently). Both results can be obtained using standard methods, 
but seem to be not present in prior work.

\section{Geometric concepts}

We revise the concepts that were designed for nearest neighbor queries
and metric approximation in metric spaces with low dimensionality.
We will also review the computation methods and how they depend
on the predicate ``$\metric(p_i,p_j)\leq t$?'' mentioned above.

\paragraph{Notion of Dimensionality.}
%
Exansion constant, doubling dimension.

\subsection{Cover trees}

\subsection{WSPD}

\subsection{Spanners}

\section{Distance Bounds in Partially Revealed Metrics}
%
We describe our algorithm to maintain lower and upper bounds for the
distance of any two points in our (finite) metric space $(P=\{p_1,\ldots,p_n\},\metric)$. We assume for simplicity that all points in $P$ are distinct.

We maintain an $n\times n$ matrix $M$ whose entries
are intervals $M_{ij}=[a_{ij},b_{ij}]$, meaning that the distance
of $p_i$ and $p_j$ lies in that interval. 
We initialize $M_{ij}=[0,\infty]$ for all $i\neq j$
and $M_{ij}=[0,0]$ for $i=j$.
We describe next how to update $M$ when a new distance has been computed.

Note that an explicit graph representation of the partially revealed metric,
such as in Figure~\ref{fig:1st_example} is not necessary. However, for 
the sake of clarity,
this graph is helpful. So, let $G=(P,E)$ be the undirected weighted graph
where an edge between $p_i$ and $p_j$ with weight $\metric(p_i,p_j)$
exists if that distance has been computed. 
We maintain the invariant that the upper bound $b_{ij}$ equals the 
shortest path from $p_i$ to $p_j$ in $G$.

Assume that $\metric(p_i,p_j)=v\in\R$ has been computed during the algorithm.
First, we reset $M_{i,j}$ and $M_{j,i}$ to $[v,v]$. 
In $G$, the edge $(p_i,p_j)$ is added with weight $v$.
To update the upper bound of some entry $M_{k,\ell}$,
we observe that the shortest path from $p_k$ to $p_\ell$ might now
go through the new edge. Hence, we update
%
\[b_{k,\ell}\gets \min\{b_{k,\ell},b_{k,i}+v+b_{j,\ell},b_{k,j}+v+b_{i,\ell}\}\]
%
Repeating this for all $k,\ell$ yields the updated upper bounds in $O(n^2)$
time.

For the lower bound, we observe that for any $1\leq k,\ell\leq n$,
%
\[v-b_{k,i}-b_{\ell,j}\]
%
is a lower bound for $\metric(p_k,p_\ell)$. Indeed, this follows from
the triangle inequality
%
\[\metric(p_i,p_j)\leq \metric(p_i,p_k)+\metric(p_k,p_\ell)+\metric(p_\ell,p_j)\]
by rearranging terms and plugging in the upper bounds for $\metric(p_i,p_k)$
and $\metric(p_k,p_\ell)$. An analogue bound holds for the roles of $i$
and $j$ swapped. Hence, we update
%
\[a_{k,\ell}\gets \max\{a_{k,\ell},v-b_{k,i}-b_{\ell,j},v-b_{k,j}-b_{\ell,i}\}\]
%
for all $k,\ell$ and obtain the updated $M$ in $O(n^2)$ time.

\medskip

With the matrix $M$, we can define filtered versions of our geometric
algorithms very easily: whenever the algorithm makes a query of the form
``Is $\metric(p_i,p_j)\leq t$?'', it is checked whether $a_{ij}> t$,
in which case the answer is ``yes'', or whether $b_{ij}\leq t$,
in which case the answer is ``no''. If none of the cases applies, 
$v\gets\metric(p_i,p_j)$ is computed, 
the matrix $M$ is updated as described above, and the predicate is answered
by comparing $v$ and $t$.

\section{Experiments on Point Clouds}
\label{sec:experiments_point_cloud}

\section{Metric spaces of Topological Summaries}
\label{sec:metric_spaces_top_summaries}

\section{Experiments on Persistence Diagrams}
\label{sec:experiments_persistence_diagrams}

\section{Conclusion}
\label{sec:conclusion}


\bibliography{bib}
\bibliographystyle{plain}


\end{document}

